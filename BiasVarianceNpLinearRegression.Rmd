---
title: "Farewell to Bias-Variance Tradeoff: A study with linear regression"
author: "Jean Ortega"
date: "12/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Goal

This is an attempt to reproduce some of the ideas in: Dar, Yehuda, Vidya Muthukumar, and Richard G Baraniuk. 2021. “A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning.” arXiv Preprint arXiv:2109.02355.

Their paper confirms you can see double descent even in linear regression-- no deep learning needed!  And they prove how model mis-specification is a big part in making this phenomenon possible, among other findings.

My approach in reproducing results was inspired by this blog: https://m-clark.github.io/posts/2021-10-30-double-descent/

## Set up

Load some custom functions.
```{r}
source("src/bias_variance.R")
```

Libraries to slice and dice and plot.

```{r}
library(tidyverse)
library(ggplot2)
```

Get the data
```{r}
data(mtcars)
```

Below w set up our X matrix of input data (replacing the y column with a column of 1's for the intercept).  Then we set up the y vector to be predicted, which is miles per gallon per car.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y.
y <- mtcars$mpg # mpg is the first column
```


## Sampling function

Here is a simple resampling function to generate training and testing errors.  It randomly select n_train rows of X and y, and p columns of X, fit a ridgeless regression, and then return the root mean squared error on the training data.  The test data uses the non-selected rows (which may be more or less than n_train).

As for regression, the Dar et al. paper used gradient descent to fit a plane in the overparameterized regime.  But here I follow Michael Clark's blog in using "rigdgeless regression," which uses the pseudo inverse of the design matrix.

```{r}
# Samples n_train rows and p columns from X for X_train and y_train.
# The remaining rows are used for X_test and y_test.
# Then trains ridgeless regression on the training data and tests on test data.
# Returns rmse on train, rmse on test (where rmse = root mean square error).
# Note: This will add intercept to X_train for you.
sample_train_test <- function(X, y, n_train, p) {
  out <- splitTrainTest(X, y, n_train, p, add_intercept = TRUE)
  result <- fit_ridgeless(out$X_train, out$y_train, out$X_test, out$y_test)
  return(c(pluck(result, 'rmse_train'), pluck(result, 'rmse_test')))
}
```

Let's also make all code after here reproducible with this seed.  Feel free to change / remove so you can run your own experiments.

```{r}
set.seed(1)
```


## Basic bias-variance

There are only 10 columns in the mtcars data set (including the intercept).  Since p, the number of parameters, can only range to 10, we can take at most 10 rows of data.  For each sample of rows and columns, we calculate train and test error.  We do this for 20 samples.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y.
num_rows <- 10 # Restricted to the number of columns, 10.
num_samples <- 40
df <- data.frame()
for (p in 2:9) {  # Only go up to p=9 because intercept will be 10th.
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p))
  df_local <- cbind(rep(p, num_rows), t(out))
  df <- rbind(df, df_local)
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

To better illustrate what this code does, let's plot every single sample.  We can see the primary impact of getting near p=10 is to increase the variance– some estimates are good but some are crazy!

```{r bias-variance1, echo=FALSE}
ggplot(df_long, aes(p, value, color=name), group_by(name)) + geom_point()
```

Taking the mean error at each amount of p, we recover the usual bias-variance diagram, showing train error always goes down by test error has a sweet spot– a minimum– somewhere in between 0 and 10.

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line()
```

Train error does go to zero at p=9, when interpolation is possible.  Meanwhile, test error is minimized at p=4, well below that point.

## Overparameterization

But of course the interesting part is what happens to overparameterized linear regression!
There are several ways to change the data set to let us see the overparameterized regime-- reducing the number of examples/row or adding additional columns.

### Reducing number of rows sampled to move interpolation threshold lower

One idea to see the overparameterized regime is to lower the number of rows.  Then we hit the overparameterized regime sooner!  For example, if num_rows is 5, then interpolation is achieved at p=5, and after that it is overparametrized.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y.
num_rows <- 5 # Lowered so we can see overparametrized regime.
num_samples <- 100
df <- data.frame()
for (p in 2:(ncol(X)-1)) {
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p))
  df_local <- cbind(rep(p, num_rows), t(out))
  df <- rbind(df, df_local)
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

Here is the plot of the mean train and test error.
```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line()
```

Training error is essentially zero after p=5.  (There can be rounding error in the pseudo-inverse function that keeps it from being exactly zero.)  Test error peaks at p=5, as expected, and then goes down, although slowly.  We have recovered double descent, but it would be good to have more parameters to see if test error in the overparameterized regime can be lower than in the regular regime. 

### Adding columns with random data

Okay, now let's go back to the most straightforward solution of adding columns.  I will start by just adding 20 columns with normally distributed random data.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y
num_rand <- 20
X <- cbind(X, matrix(rnorm(num_rand*nrow(X), 0, 1), nrow=nrow(X), ncol=num_rand))
```

```{r}
num_rows <- 10 # Back to 10
num_samples <- 40
df <- data.frame()
for (p in 2:29) { # Increased range of parameters.
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p))
  df_local <- cbind(rep(p, num_rows), t(out))
  df <- rbind(df, df_local)
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

So how did this work?  I expect higher error overall with all those random columns, with a max error at p = 10.

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line() + geom_point()
```

Wow!  The theoretical results are validated!  Up to p=10, we got a fairly straightforward bias-variance diagram.  After tha, train error is zero, as expected, once interpolation is reached.  Test error slowly decreases, even going lower than what we saw in the p < 10 regime.

### Adding columns with 2nd order effects

Okay, now let's try another way of adding columns.  Let's try every combination of pairs of columns.  (I could automate the column generation, but I'm going to do the most obvious way so you can see there is not a bug in how I created the data.)

```{r}
X <- as.matrix(mtcars[, -1])
X <- cbind(X,
  X[,1]*X[,1], X[,1]*X[,2], X[,1]*X[,3], X[,1]*X[,4], X[,1]*X[,5], X[,1]*X[,6], X[,1]*X[,7], X[,1]*X[,8], X[,1]*X[,9],
  X[,2]*X[,1], X[,2]*X[,2], X[,2]*X[,3], X[,2]*X[,4], X[,2]*X[,5], X[,2]*X[,6], X[,2]*X[,7], X[,2]*X[,8], X[,2]*X[,9],
  X[,3]*X[,1], X[,3]*X[,2], X[,3]*X[,3], X[,3]*X[,4], X[,3]*X[,5], X[,3]*X[,6], X[,3]*X[,7], X[,3]*X[,8], X[,3]*X[,9],
  X[,4]*X[,1], X[,4]*X[,2], X[,4]*X[,3], X[,4]*X[,4], X[,4]*X[,5], X[,4]*X[,6], X[,4]*X[,7], X[,4]*X[,8], X[,4]*X[,9],
  X[,5]*X[,1], X[,5]*X[,2], X[,5]*X[,3], X[,5]*X[,4], X[,5]*X[,5], X[,5]*X[,6], X[,5]*X[,7], X[,5]*X[,8], X[,5]*X[,9],
  X[,6]*X[,1], X[,6]*X[,2], X[,6]*X[,3], X[,6]*X[,4], X[,6]*X[,5], X[,6]*X[,6], X[,6]*X[,7], X[,6]*X[,8], X[,6]*X[,9],
  X[,7]*X[,1], X[,7]*X[,2], X[,7]*X[,3], X[,7]*X[,4], X[,7]*X[,5], X[,7]*X[,6], X[,7]*X[,7], X[,7]*X[,8], X[,7]*X[,9],
  X[,8]*X[,1], X[,8]*X[,2], X[,8]*X[,3], X[,8]*X[,4], X[,8]*X[,5], X[,8]*X[,6], X[,8]*X[,7], X[,8]*X[,8], X[,8]*X[,9],
  X[,9]*X[,1], X[,9]*X[,2], X[,9]*X[,3], X[,9]*X[,4], X[,9]*X[,5], X[,9]*X[,6], X[,9]*X[,7], X[,9]*X[,8], X[,9]*X[,9]
  )
```

```{r}
num_rows <- 10 # Back to 10
num_samples <- 40
df <- data.frame()
for (p in 2:90) { # 9 original + 9 * 9, intercept added later
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p))
  df_local <- cbind(rep(p, num_rows), t(out))
  df <- rbind(df, df_local)
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line() + geom_point()
```

Ugly.  I don't see the original bias-variance relationship with p < 10.  Train error should be near zero at p=10.  And test error just keeps going up.  I will look for a bug in my code.

## References

Dar, Yehuda, Vidya Muthukumar, and Richard G Baraniuk. 2021. “A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning.” arXiv Preprint arXiv:2109.02355.
