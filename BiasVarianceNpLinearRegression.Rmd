---
title: "Farewell to Bias-Variance Tradeoff: A study with linear regression"
author: "Jean Czerlinski Ortega"
date: "12/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Goal

This is an attempt to reproduce some of the ideas in: Dar, Yehuda, Vidya Muthukumar, and Richard G Baraniuk. 2021. “A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning.” arXiv Preprint arXiv:2109.02355.

Their paper confirms you can see double descent even in linear regression-- no deep learning needed!  And they prove how model mis-specification is a big part in making this phenomenon possible, among other findings.

My approach in reproducing results was inspired by this blog: https://m-clark.github.io/posts/2021-10-30-double-descent/

## Set up

Load some custom functions.
```{r}
source("src/bias_variance.R")
```

Libraries to slice and dice and plot.

```{r}
library(tidyverse)
library(ggplot2)
```

Get the data
```{r}
data(mtcars)
```


## Relevant functions

Below is the simple resampling function to generate training and testing data.

* It randomly selects n_train rows of X and y
* The remaining rows are used for test data for X and y
* I also randomely selects p columns of X for use in both train and test.

```{r}
splitTrainTest
```


Below I wrote a function to use the above sampler to generate data, then use a funtion that fits a linear regression and returns the root mean squared error on the training data and test data.

Note that the Dar et al. paper used gradient descent to fit the linear model, but here I follow Michael Clark's blog in using rigdeless regression, which uses the pseudo inverse of the design matrix.  I can get the same effects with a lot less computation time.  (Ridgeless regression was also analytically studied in Hastie et al. 2019).

```{r}
# Samples n_train rows and p columns from X for X_train and y_train.
# The remaining rows are used for X_test and y_test.
# Then trains ridgeless regression on the training data and measures error on both data sets.
# Returns # parameters, rmse on train, rmse on test (where rmse = root mean square error).
# Note: # parameters will be p if add_intercewpt is false and p+1 if add_intercept is true.
sample_train_test <- function(X, y, n_train, p, add_intercept = TRUE) {
  out <- splitTrainTest(X, y, n_train, p, add_intercept)
  result <- fit_ridgeless(out$X_train, out$y_train, out$X_test, out$y_test)
  # number of parameters, rmse on train, rmse on test
  return(c(ncol(out$X_train), pluck(result, 'rmse_train'), pluck(result, 'rmse_test')))
}
```

Let's also make all code after here reproducible with this seed.  Feel free to change / remove so you can run your own experiments.

```{r}
set.seed(1)
```


## Basic bias-variance

Below we set up our X matrix of input data, removing the miles per gallon column because that is what we will prdected.  (The intercept will be added later.)  Then we set up the y vector to be predicted.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y.
y <- mtcars$mpg # mpg is the first column
```

There are only 9 columns in the X(after removing mpg) and we will add the intercept in code, resulting in a total of 10 predictors, so we can take at most 10 rows of data to stay within the underparametrized regime.  For each sample of rows and columns, we calculate train and test error.  We do this for 40 samples.


```{r}
num_rows <- 10 # Because X has 9 columns plus the intercept.
num_samples <- 40
df <- data.frame()
for (p in 2:9) {  # Only go up to p=9 because intercept will be 10th.
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p, add_intercept = TRUE))
  df <- rbind(df, t(out))
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

To better illustrate what this code does, let's plot every single sample.  We can see the primary impact of getting near p=10 is to increase the variance– some estimates are good but some are crazy!

```{r bias-variance1, echo=FALSE}
ggplot(df_long, aes(p, value, color=name), group_by(name)) + geom_point()
```

Taking the mean error at each amount of p, we recover the usual bias-variance diagram, showing train error always goes down by test error has a sweet spot– a minimum– somewhere in between 0 and 10.

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line()
```

Train error does go to zero at p=9, when interpolation is possible.  Meanwhile, test error is minimized at p=4, well below that point.

## Overparameterization

But of course the interesting part is what happens to overparameterized linear regression!
There are several ways to change the data set to let us see the overparameterized regime-- reducing the number of examples/row or adding additional columns.

### Adding columns with random data

Let's try the easiest way to add columns: add random data.  I will add 20 columns with normally distributed random data.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y
num_rand <- 20
X <- cbind(X, matrix(rnorm(num_rand*nrow(X), 0, 1), nrow=nrow(X), ncol=num_rand))
```

```{r}
num_rows <- 10
num_samples <- 40
df <- data.frame()
for (p in 2:29) { # Increased range of parameters.
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p, add_intercept = TRUE))
  df <- rbind(df, t(out))
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

So how did this work?  I expect higher error overall with all those random columns, with a max error at p = 10.

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line() + geom_point()
```

Wow!  The theoretical results are validated!  Up to p=9-- meaning p=10 with the intercept-- we got a fairly standard bias-variance diagram: train error goes down while test error goes up.  For p>=9, train error is zero, as expected, because the model can interpolate the data.  For p >= 9, test error slowly decreases, showing double descent, eventually going lower than the lowest error we saw in the p < 10 regime.

However, I am shocked that the **global** minimum is attained in the regime where extra **purely random** columns are added.  I am familiar with the idea that noise can act like a regularizer, but usually it is injected as "real data + noise" (among other ways).  But I am shocked that pure noise columns have this beneficial effect.

### Reducing number of rows sampled to move interpolation threshold lower

Another idea to see the overparameterized regime is to lower the number of rows.  Then we hit the overparameterized regime for lower p.  For example, if num_rows is 4, then interpolation is achieved at p=3 + 1 intercent, and for higher p it is overparametrized.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y.
num_rows <- 4 # Lowered so we can see overparametrized regime.
num_samples <- 200
df <- data.frame()
for (p in 2:(ncol(X)-1)) {
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p, add_intercept = TRUE))
  df <- rbind(df, t(out))
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

Here is the plot of the mean train and test error.
```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line()
```

Training error is mostly zero after p=4.  (There can be rounding error in the pseudo-inverse function that keeps it from being exactly zero.)  Test error peaks close to p=3 (but why p=4?) and then goes down, although slowly.  We have recovered double descent without random data, although the limit on number of parameters does not let us explore it much. 


### Adding columns with 2nd order effects

Okay, now let's try another way of adding columns.  Let's try every combination of pairs of columns.  (I could automate the column generation, but I'm going to do the most obvious way so you can see there is not a bug in how I created the data.)

```{r}
X <- as.matrix(mtcars[, -1])
X <- cbind(X,
  X[,1]*X[,1], X[,1]*X[,2], X[,1]*X[,3], X[,1]*X[,4], X[,1]*X[,5], X[,1]*X[,6], X[,1]*X[,7], X[,1]*X[,8], X[,1]*X[,9],
               X[,2]*X[,2], X[,2]*X[,3], X[,2]*X[,4], X[,2]*X[,5], X[,2]*X[,6], X[,2]*X[,7], X[,2]*X[,8], X[,2]*X[,9],
                           X[,3]*X[,3], X[,3]*X[,4], X[,3]*X[,5], X[,3]*X[,6], X[,3]*X[,7], X[,3]*X[,8], X[,3]*X[,9],
                                         X[,4]*X[,4], X[,4]*X[,5], X[,4]*X[,6], X[,4]*X[,7], X[,4]*X[,8], X[,4]*X[,9],
                                                      X[,5]*X[,5], X[,5]*X[,6], X[,5]*X[,7], X[,5]*X[,8], X[,5]*X[,9],
                                                                   X[,6]*X[,6], X[,6]*X[,7], X[,6]*X[,8], X[,6]*X[,9],
                                                                                X[,7]*X[,7], X[,7]*X[,8], X[,7]*X[,9],
  X[,8]*X[,8], X[,8]*X[,9],
  X[,9]*X[,9]
  )
```

```{r}
num_rows <- 10
num_samples <- 20
df <- data.frame()
for (p in 2:45) { # 9 original + 9 * 9, intercept added later
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p, add_intercept = TRUE))
  df <- rbind(df, t(out))
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line() + geom_point()
```

Huh?  I don't see the original bias-variance relationship with p < 10.  Train error should be near zero at p=10.  And test error just keeps going up.  I will look for a bug in my code.

## References

Clark, Michael. (2021, Nov. 13). Double Descent. Retrieved from https://m-clark.github.io/posts/2021-10-30-double-descent/

Dar, Yehuda, Vidya Muthukumar, and Richard G Baraniuk. 2021. “A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning.” arXiv Preprint arXiv:2109.02355.

Hastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. 2019. “Surprises in High-Dimensional Ridgeless Least Squares Interpolation.” arXiv Preprint arXiv:1903.08560.
