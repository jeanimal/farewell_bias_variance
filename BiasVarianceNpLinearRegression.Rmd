---
title: "Farewell to Bias-Variance Tradeoff: A study with linear regression"
author: "Jean Czerlinski Ortega"
date: "12/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Goal

This is an attempt to reproduce some of the ideas in: Dar, Yehuda, Vidya Muthukumar, and Richard G Baraniuk. 2021. “A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning.” arXiv Preprint arXiv:2109.02355.

Their paper confirms you can see double descent even in linear regression-- no deep learning needed!  And they prove how model mis-specification is a big part in making this phenomenon possible, among other findings.

My approach in reproducing results was inspired by this blog: https://m-clark.github.io/posts/2021-10-30-double-descent/

## Set up

Load some custom functions.
```{r}
source("src/bias_variance.R")
```

Libraries to slice and dice and plot.

```{r}
library(tidyverse)
library(ggplot2)
```

Get the data
```{r}
data(mtcars)
```


## Relevant functions

Below is the simple resampling function to generate training and testing data.

* It randomly selects n_train rows of X and y
* The remaining rows are used for test data for X and y
* I also randomely selects p columns of X for use in both train and test.

```{r}
splitTrainTest
```


Below I wrote a function to use the above sampler to generate data, then use a funtion that fits a linear regression and returns the root mean squared error on the training data and test data.

Note that the Dar et al. paper used gradient descent to fit the linear model, but here I follow Michael Clark's blog in using ridgeless regression to get the same effects with a lot less computation time.  (Ridgeless regression uses the pseudo inverse of the design matrix but gets its name because it is equivalent to using ridge regression in the limit as the lambda parameter goes to zero.  The double descent behavior of ridgeless regression was also analytically studied in Hastie et al. 2019).

```{r}
# Samples n_train rows and p columns from X for X_train and y_train.
# The remaining rows are used for X_test and y_test.
# Then trains ridgeless regression on the training data and measures error on both data sets.
# Returns # parameters, rmse on train, rmse on test (where rmse = root mean square error).
# Note: # parameters will be p if add_intercewpt is false and p+1 if add_intercept is true.
sample_train_test <- function(X, y, n_train, p, add_intercept = TRUE) {
  out <- splitTrainTest(X, y, n_train, p, add_intercept)
  result <- fit_ridgeless(out$X_train, out$y_train, out$X_test, out$y_test)
  # number of parameters, rmse on train, rmse on test
  return(c(ncol(out$X_train), pluck(result, 'rmse_train'), pluck(result, 'rmse_test')))
}
```

Let's also make all code after here reproducible with this seed.  Feel free to change / remove so you can run your own experiments.

```{r}
set.seed(1)
```


## Basic bias-variance

Below we set up our X matrix of input data, removing the miles per gallon column because that is what we will prdected.  (The intercept will be added later.)  Then we set up the y vector to be predicted.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y.
y <- mtcars$mpg # mpg is the first column
```

There are only 9 columns in the X(after removing mpg) and we will add the intercept in code, resulting in a total of 10 predictors, so we can take at most 10 rows of data to stay within the underparametrized regime.  For each sample of rows and columns, we calculate train and test error.  We do this for 40 samples.


```{r}
num_rows <- 10 # Because X has 9 columns plus the intercept.
num_samples <- 40
df <- data.frame()
for (p in 2:9) {  # Only go up to p=9 because intercept will be 10th.
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p, add_intercept = TRUE))
  df <- rbind(df, t(out))
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

To better illustrate what this code does, let's plot every single sample.  We can see the primary impact of getting near p=10 is to increase the variance– some estimates are good but some are crazy!

```{r bias-variance1, echo=FALSE}
ggplot(df_long, aes(p, value, color=name), group_by(name)) + geom_point()
```

Taking the mean error at each amount of p, we recover the usual bias-variance diagram, showing train error always goes down by test error has a sweet spot– a minimum– somewhere in between 0 and 10.

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line()
# Save for later.
mean_error_mtcars <- df_mean
```

Train error does go to zero at p=9, when interpolation is possible.  Meanwhile, test error is minimized at p=4, well below that point.

## Overparameterization

But of course the interesting part is what happens to overparameterized linear regression!
There are several ways to change the data set to let us see the overparameterized regime-- reducing the number of examples/row or adding additional columns.

### Adding columns with random data

Let's try the easiest way to add columns: add random data.  I will add 20 columns with normally distributed random data.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y
num_rand <- 20
X <- cbind(X, matrix(rnorm(num_rand*nrow(X), 0, 1), nrow=nrow(X), ncol=num_rand))
```

```{r}
num_rows <- 10
num_samples <- 40
df <- data.frame()
for (p in 2:29) { # Increased range of parameters.
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p, add_intercept = TRUE))
  df <- rbind(df, t(out))
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

So how did this work?  I expect higher error overall with all those random columns, with a max error at p = 10.

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line() + geom_point()
# Save for later.
mean_error_mtcars_with_random <- df_mean
```

Wow!  The theoretical results are validated!  Up to p=9-- meaning p=10 with the intercept-- we got a fairly standard bias-variance diagram: train error goes down while test error goes up.  For p>=9, train error is zero, as expected, because the model can interpolate the data.  For p >= 9, test error slowly decreases, showing double descent, eventually going lower than the lowest error we saw in the p < 10 regime.

#### Interpretation

Notice that the **global** test error minimum is attained in the regime with more columns-- at p=30, meaning all the parameters  However, look at the amount of minimum test error attained, just under 5.  If you look back at the bias-variance diagram from the fit where only mtcars columns were used (no random columns), the minimum test error attained was just under 4.  In other words, the lowest test error was achieved when the fit had just the substantive parameters-- that is, a well-specified model-- without all the extra noise columns.  A small model with well-specified parameters attained better test error than a large model with a grab-bag of parameters.  Here is a graph that focuses on these two cases.

```{r}
df_mean <- rbind(
  cbind(mean_error_mtcars, features="mtcars"),
  cbind(mean_error_mtcars_with_random, features="mtcars with random")
)
```

```{r}
df_mean_test_error <- df_mean %>% filter(name == "test")
ggplot(df_mean_test_error, aes(p, mean_error, color=features), group_by(features)) + geom_line() + geom_point() + ylab("mean test error")
```

That said, in real life we do not always know or have access to just the substantive parameters.  And in the internet age, we can often access a lot of low-signal parameters easily by scraping the internet.  In that case, collecting more and more features is helpful.  As Dar et al. point out, this is akin to taking an ensemble of models or boosting or the strength of weak learners.  

Below is an example of the grab-bag of parameters we get for different values of p.  Columns with a name are original to the mtcars data set.  Columns without a name contain random data.  As you can see, the higher p is, the more columns of useful mtcars data we get.

*P=5*
```{r}
colnames(splitTrainTest(X, y, n_train=2, p=5, add_intercept=FALSE)$X_train)
```

*P=10*
```{r}
colnames(splitTrainTest(X, y, n_train=2, p=10, add_intercept=FALSE)$X_train)
```

*P=20*
```{r}
colnames(splitTrainTest(X, y, n_train=2, p=20, add_intercept=FALSE)$X_train)
```


Clearly,p=30 gets all the mtcars columns on every sample, and that's why it attains the lowest test error.  (Perhaps I should have used sampling with replacement to avoid this artificial effect.)

#### Characteritics of the data set

At first, I was surprised that adding purely random cues demonstrated double descent. Although there are quite a few papers explaining how noise can act like a regularizer,  usually noise is injected as additional rows of "real data + noise" (among other ways).  So I was surprised that pure noise columns also had an interesting effect.

But re-reading the Dar et al. paper, I noticed these two (of three) conditions in section 3.1.3 for double descent:

1. low effective dimension in data
2. an overparameterized number of low-value (but non-zero signal) features

Clearly the data set has low effective deimsnion when there are 10 mtcars columns (which should mostly have signal) and 20 random columns.  The second condition is met because sampling p of these 30 columns has an 0.3 probability of getting an mtcars column (which is signal) as opposed to a non-signal random column.  The "low but non-zero signal value" applies to the expectation for the column in the data set rather than requiring each column individually to have a low but non-zero signal value.

The other data sets I tried did not clearly meet these two conditions, and they failed to show double descent.

### Reducing number of rows sampled to move interpolation threshold lower

Another idea to see the overparameterized regime is to lower the number of rows.  Then we hit the overparameterized regime for lower p.  For example, if num_rows is 4, then interpolation is achieved at p=3 + 1 intercent, and for higher p it is overparametrized.

```{r}
X <- as.matrix(mtcars[, -1]) # Drop mpg (1st col) because it is y.
num_rows <- 4 # Lowered so we can see overparametrized regime.
num_samples <- 200
df <- data.frame()
for (p in 2:(ncol(X)-1)) {
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p, add_intercept = TRUE))
  df <- rbind(df, t(out))
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

Here is the plot of the mean train and test error.
```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line()
```

Training error is mostly zero after p=4.  (There can be rounding error in the pseudo-inverse function that keeps it from being exactly zero.)  Test error peaks close to p=3 (but why p=4?) and then goes down, although slowly.  We have recovered double descent without random data, although the limit on number of parameters does not let us explore it much. However, it is quite possible that the lowest error would be in underparameterized regime because these two conditions are not clearly satisfied:

1. low effective dimension in data
2. an overparameterized number of low-value (but non-zero signal) features

### Adding columns with 2nd order effects

Okay, now let's try another way of adding columns.  Let's try every combination of pairs of columns.  (I could automate the column generation, but I'm going to do the most obvious way so you can see there is not a bug in how I created the data.)

```{r}
X <- as.matrix(mtcars[, -1])
X <- cbind(X,
  X[,1]*X[,1], X[,1]*X[,2], X[,1]*X[,3], X[,1]*X[,4], X[,1]*X[,5], X[,1]*X[,6], X[,1]*X[,7], X[,1]*X[,8], X[,1]*X[,9],
               X[,2]*X[,2], X[,2]*X[,3], X[,2]*X[,4], X[,2]*X[,5], X[,2]*X[,6], X[,2]*X[,7], X[,2]*X[,8], X[,2]*X[,9],
                           X[,3]*X[,3], X[,3]*X[,4], X[,3]*X[,5], X[,3]*X[,6], X[,3]*X[,7], X[,3]*X[,8], X[,3]*X[,9],
                                         X[,4]*X[,4], X[,4]*X[,5], X[,4]*X[,6], X[,4]*X[,7], X[,4]*X[,8], X[,4]*X[,9],
                                                      X[,5]*X[,5], X[,5]*X[,6], X[,5]*X[,7], X[,5]*X[,8], X[,5]*X[,9],
                                                                   X[,6]*X[,6], X[,6]*X[,7], X[,6]*X[,8], X[,6]*X[,9],
                                                                                X[,7]*X[,7], X[,7]*X[,8], X[,7]*X[,9],
  X[,8]*X[,8], X[,8]*X[,9],
  X[,9]*X[,9]
  )
```

```{r}
num_rows <- 10
num_samples <- 20
df <- data.frame()
for (p in 2:45) { # 9 original + 9 * 9/2, intercept added later
  out <- replicate(n=num_samples, sample_train_test(X, y, num_rows, p, add_intercept = TRUE))
  df <- rbind(df, t(out))
}
colnames(df) <- c("p", "train", "test")
df_long <- pivot_longer(df, train:test)
```

```{r}
df_mean <- df_long %>% group_by(p, name) %>% summarize(mean_error=mean(value))
ggplot(df_mean, aes(p, mean_error, color=name), group_by(name)) + geom_line() + geom_point()
```

I do not see the original bias-variance relationship with p < 10.  Train error should be near zero at p=10.  This might indicate a bug in the code, but so far I have not found one.

As for the test error going up and up, that is not necessarily a bug.  Adding columns with second order effects would not satisfy these conditions:

1. low effective dimension in data
2. an overparameterized number of low-value (but non-zero signal) features

## References

Clark, Michael. (2021, Nov. 13). Double Descent. Retrieved from https://m-clark.github.io/posts/2021-10-30-double-descent/

Dar, Yehuda, Vidya Muthukumar, and Richard G Baraniuk. 2021. “A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning.” arXiv Preprint arXiv:2109.02355.

Hastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. 2019. “Surprises in High-Dimensional Ridgeless Least Squares Interpolation.” arXiv Preprint arXiv:1903.08560.
